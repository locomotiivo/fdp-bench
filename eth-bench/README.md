# FDP Ethereum Benchmark

Measures the impact of NVMe **Flexible Data Placement (FDP)** on Ethereum
execution-layer performance using a post-merge testnet (Geth EL + Lighthouse CL)
with realistic workloads generated by [EthPandaOps spamoor](https://github.com/ethpandaops/spamoor).

## Quick Start

```bash
# One-time setup (downloads Lighthouse, generates validator keys)
./scripts/setup_testnet.sh

# Run full FDP vs non-FDP comparison (~2.5 hours)
./scripts/bench.sh
```

## Directory Structure

```
eth-bench/
├── README.md                  # This file
├── scripts/
│   ├── bench.sh               # Synthetic workload orchestrator (spamoor-based)
│   ├── replay.sh              # Mainnet block replay benchmark
│   ├── env.sh                 # Ethereum-specific config (geth/LH paths, PID mapping)
│   ├── helpers.sh             # Shared functions (parsing, metrics, spamoor, lifecycle)
│   ├── run_geth.sh            # Start geth (EL) with FDP-aware Pebble
│   ├── run_lighthouse.sh      # Start Lighthouse (CL) beacon + validator
│   ├── gen_genesis.sh         # Generate EL genesis.json + CL genesis.ssz
│   └── setup_testnet.sh       # One-time: install binaries + gen validator keys
├── era1/                      # Downloaded Era1 files + blocks.rlp
├── testnet/                   # CL config, genesis, validator keystores
├── results/                   # Synthetic benchmark output
│   ├── non-fdp/
│   └── fdp/
└── results-replay/            # Mainnet replay output
    ├── non-fdp/
    └── fdp/
```

Shared infra lives one level up:
```
fdp-scripts/
├── common/
│   ├── env.sh                 # Device paths, mount, SSH, fdp_stats
│   ├── fdp_stats              # FEMU FTL stats binary
│   └── fdp_stats.cc           # Source
├── f2fs-tools-fdp/            # mkfs.f2fs + fdp_f2fs_mount
├── eth-bench/                 # ← you are here
└── sui-bench/
```

## Methodology

### Configuration vs Ethereum Mainnet

| Parameter | Mainnet | Bench | Justification |
|-----------|---------|-------|---------------|
| Slot time | 12 s | 4 s | Eliminates dead p2p propagation time |
| Block gas limit | 30 M | 500 M | ~17× gas per block for time compression |
| State snapshots | enabled | disabled | Forces reads through MPT on disk |
| Pebble memtable | 256 MB | 64 MB | Resource-constrained validator model |
| Pebble compression | Snappy | off | Deterministic WAF measurement |

**Net effect:** ~50× mainnet I/O throughput.  The I/O *pattern* (write mix,
access distribution, read/write ratio) is unchanged; only the *rate* is
compressed in time.

### Workload Phases (Realistic Mode)

| Phase | Duration | Scenario | Purpose |
|-------|----------|----------|---------|
| 1. Seed | 55 min | storagespam | Fill SSD past GC threshold |
| 2. DeFi | 10 min | uniswap-swaps + bg storagespam | **MEASURED** window under GC |
| 3. Burst | 2 min | erc20tx | Simulates mempool event |

### FDP Stream Allocation (8 PIDs)

| PID | Directory | Temperature | Content |
|-----|-----------|-------------|---------|
| p0 | wal/ | HOTTEST | Pebble WAL |
| p1 | sst_flush/ | HOT | Pebble flush L0 SSTs |
| p2 | sst_l0cmp/ | WARM | L0→Lbase compaction SSTs |
| p3 | chaindata/ | COOL | Deep SSTs + MANIFEST |
| p4 | cl_hot/ | HOT | Lighthouse hot DB |
| p5 | cl_cold/ | COOL | Lighthouse freezer DB |
| p6 | ancient/ | COLDEST | EL frozen block archive |
| p7 | meta/ | COLDEST | Metadata + logs |

## Metrics Collected

- **Confirmed TPS** — On-chain transactions per second (disk-sensitive)
- **Throughput** — tx/block (spamoor 60-block average)
- **WAF** — Write Amplification Factor (from FEMU FTL counters)
- **Blocks/sec** — Block production rate
- **Write Stalls** — Pebble stall count & duration
- **Compaction Time** — Total Pebble compaction wall-clock time
- **L0 / Non-L0 Compactions** — Compaction event counts

## Mainnet Block Replay (`replay.sh`)

Replays real Ethereum mainnet blocks through `geth import` (full EVM
re-execution), following the methodology of academic papers:

- **LVMT** (OSDI '23) — replayed blocks 0-15M
- **Ethanos** (EuroSys '21) — replayed blocks 0-9M
- **LMPTs** (IEEE ICBC '22) — replayed ~14M blocks

### Why Replay?

The synthetic spamoor benchmark (`bench.sh`) is useful for measuring
throughput under controlled conditions, but replay provides:

1. **Real state growth patterns** — Ethereum's actual account/storage
   distribution and access patterns
2. **Academic comparability** — Same methodology as published results
3. **Steady-state GC** — Continuous state growth naturally fills the SSD

### Quick Start

```bash
# 1. Download Era1 files (blocks packed in 8192-block epochs)
./scripts/replay.sh download 0 600000

# 2. Convert to RLP (pure file-to-file, no DB needed)
./scripts/replay.sh convert

# 3. Run FDP vs non-FDP replay
./scripts/replay.sh bench
```

### How It Works

1. **Download**: `geth download-era` fetches Era1 archives from
   era.ethportal.net with embedded SHA256 checksum verification.

2. **Convert**: `era2rlp` (custom Go tool in `go-ethereum/cmd/era2rlp/`)
   reads Era1 files and outputs concatenated RLP-encoded blocks — the
   exact format `geth import` expects. No intermediate database needed.

3. **Import**: `geth import blocks.rlp` fully re-executes every
   transaction through the EVM, writing all state transitions to Pebble.
   This is the same code path as syncing from genesis, but from local
   block data instead of p2p.

### Metrics

- **Blocks/sec** — Import throughput
- **Mgas/sec** — Gas execution rate
- **WAF** — Write Amplification Factor
- **Write Stalls** — Pebble stall count & duration
- **Compaction Time** — Total Pebble compaction time
